{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seshat_api import SeshatAPI\n",
    "import pandas as pd\n",
    "from ollama import chat, ChatResponse\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions for DeepSeek-R1\n",
    "\n",
    "Can a recent top performing LLM (DeepSeek-R1) correctly predict whether a variable from the Seshat Global History Databank (e.g. \"Scientific Literature\") should be \"present\" or \"absent\" for a selection of polities, given a definition of the variable, the name of the polity and the years in which it existed?\n",
    "\n",
    "If DeepSeek has a good understanding of history, can it use this to guess from the polity name and years what time/place the prompt is referring to, and whether the variable in question would have been present?\n",
    "\n",
    "**Important:** I have included some example CSVs of DeepSeek responses to prompts, since running hundreds of prompts took several hours on my laptop (M1 Mac). If you want to generate new responses from DeepSeek, make sure you uncomment the calls to the `deepseek_responses` function below, but also make sure to *not* overwrite your own responses in the next cell where I load the pre-made responses from CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = SeshatAPI(base_url=\"https://seshat-db.com/api\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data from Seshat\n",
    "\n",
    "First let's define our variable to be used in an LLM prompt later (taken from seshat-db.com), then load the data from the Seshat API to use as our ground truth to test against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = 'Scientific Literature'\n",
    "definition = \"Talking about Kinds of Written Documents, Scientific literature includes mathematics, natural sciences, social sciences\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seshat_api.sc import ScientificLiteratures\n",
    "scientific_literatures = ScientificLiteratures(client)\n",
    "scientific_literatures_df = pd.DataFrame(scientific_literatures.get_all())\n",
    "len(scientific_literatures_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scientific_literatures_df[scientific_literatures_df['new_name'] == 'tr_ottoman_emp_1']\n",
    "scientific_literatures_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(scientific_literatures_df['polity'].tolist())\n",
    "test['scientific_literature'] = scientific_literatures_df['scientific_literature']\n",
    "test[test['new_name'] == 'tr_ottoman_emp_1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just use expert reviewed data and ignore examples where the value is anything other than \"present\" or \"absent\" to create a subsample of the dataset.\n",
    "\n",
    "We should also reformat the dataframe so we have information about the polities such as the start and end year alongside the variable value, then remove columns we aren't interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(seshat_df, variable_id):\n",
    "\n",
    "    # Extract the polities column to a new dataframe\n",
    "    polities_df = pd.DataFrame(seshat_df['polity'].tolist())\n",
    "\n",
    "    # Add columns to the new dataframe\n",
    "    polities_df[variable_id] = seshat_df[variable_id]\n",
    "    polities_df['expert_reviewed'] = seshat_df['expert_reviewed']\n",
    "\n",
    "    # Filter out the records that are not expert reviewed\n",
    "    polities_df = polities_df[polities_df['expert_reviewed'] == True]\n",
    "\n",
    "    # Filter out records where the variable is not either 'present' or 'absent'\n",
    "    polities_df = polities_df[polities_df[variable_id].isin(['present', 'absent'])]\n",
    "\n",
    "    # Filter out records where the variable is NaN\n",
    "    polities_df = polities_df[polities_df[variable_id].notna()]\n",
    "    print(\"There are\", len(polities_df), variable_id, \"records after filtering.\")\n",
    "\n",
    "    # Get rid of the columns we don't need\n",
    "    polities_df = polities_df[['new_name', 'long_name', 'start_year', 'end_year', variable_id, 'general_description']]\n",
    "\n",
    "    return polities_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polities_with_scientific_literatures_df = process_df(scientific_literatures_df, 'scientific_literature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sense check\n",
    "polities_with_scientific_literatures_df[polities_with_scientific_literatures_df['new_name'] == 'tr_ottoman_emp_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polities_with_scientific_literatures_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Prompt engineering\"\n",
    "\n",
    "First let's define a prompt function to use with our dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_CE(year):\n",
    "    if year >= 0:\n",
    "        return f\"{year} CE\"\n",
    "    else:\n",
    "        return f\"{abs(year)} BCE\"\n",
    "\n",
    "def prompt_func(seshat_df, polity_name, variable, variable_definition):\n",
    "    df = seshat_df[seshat_df['new_name'] == polity_name]\n",
    "    polity = list(df['long_name'])[0]\n",
    "    # description = list(df['general_description'])[0]  # TODO: we could add this to the prompt to add more context\n",
    "    start_year = list(df['start_year'])[0]\n",
    "    end_year = list(df['end_year'])[0]\n",
    "    prompt = \"Use your knowledge of world history to answer the following question. \"\n",
    "    prompt += f\"Given your knowledge of the historical polity '{polity}', \"\n",
    "    prompt += f\"a polity that existed between {year_CE(start_year)} and {year_CE(end_year)}\"\n",
    "    prompt += f\", do you expect that {variable} was present or absent? \"\n",
    "    prompt += f\"{variable} is defined as: '{variable_definition}'. \"\n",
    "    \n",
    "    # To help extract the answer from the text response later, make sure we have a string that can be found with a regex:\n",
    "    prompt += \"Answer 'XXXpresentXXX' if you expect it to be present, and 'XXXabsentXXX' if you expect it to be absent.\"\n",
    "    return prompt\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the prompt look with an example \"new_name\" (Seshat ID) of `tn_fatimid_cal`, which we know has a record for scientific literature in the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seshat_polity_name = 'tn_fatimid_cal'\n",
    "test_prompt = prompt_func(polities_with_scientific_literatures_df, test_seshat_polity_name, variable, definition)\n",
    "test_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's see what DeepSeek responds for this prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response: ChatResponse = chat(model='deepseek-r1', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': test_prompt,\n",
    "  },\n",
    "])\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was that correct? Let's check the dataframe to see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polities_with_scientific_literatures_df[polities_with_scientific_literatures_df['new_name'] == test_seshat_polity_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get DeepSeek's answer data\n",
    "\n",
    "Let's write a function to collect responses from DeepSeek for all the polities in our dataset and make a new dataframe with the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepseek_responses(seshat_df, variable, definition, count=None):  # Use the count parameter to limit the number of responses\n",
    "    def generate_response(polity):\n",
    "        response: ChatResponse = chat(model='deepseek-r1', messages=[\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': prompt_func(seshat_df, polity, variable, definition),\n",
    "            },\n",
    "        ])\n",
    "        return response\n",
    "\n",
    "    responses = pd.DataFrame(columns=['new_name', 'answer', 'full'])\n",
    "    for polity in seshat_df['new_name']:\n",
    "        if count is not None and len(responses) >= count:\n",
    "            break\n",
    "        response = generate_response(polity)\n",
    "        while 'XXXabsentXXX' not in response.message.content and 'XXXpresentXXX' not in response.message.content:\n",
    "            response = generate_response(polity)  # Keep asking until we get a valid response\n",
    "        answer = re.search(r'(XXXabsentXXX|XXXpresentXXX)', response.message.content).group(1)\n",
    "        answer = answer.replace(\"XXX\", \"\")\n",
    "\n",
    "        # Add the new row to the DataFrame using pd.concat\n",
    "        responses = pd.concat([responses, pd.DataFrame([{\n",
    "            'new_name': polity,\n",
    "            'answer': answer,\n",
    "            'full': response.message.content\n",
    "        }])], ignore_index=True)\n",
    "\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate the answer data for this variable across polities in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the responses for the scientific literature variable - uncomment to run (you can set count to limit the number of responses)\n",
    "scientific_literature_responses = deepseek_responses(polities_with_scientific_literatures_df, variable, definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the responses to a CSV file - uncomment to run\n",
    "scientific_literature_responses.to_csv('scientific_literature_responses.csv', index=False)\n",
    "\n",
    "# Load the responses from the CSV file\n",
    "# scientific_literature_responses = pd.read_csv('scientific_literature_responses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scientific_literature_responses.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How well did the model perform?\n",
    "\n",
    "Let's do a simple check to see what percentage of the answers were correct, according to the Seshat ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance(polities_df, responses_df, variable_id):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for _, response in responses_df.iterrows():\n",
    "        polity = response['new_name']\n",
    "        seshat_answer = polities_df[polities_df['new_name'] == polity][variable_id].values[0]\n",
    "        if seshat_answer == response['answer']:\n",
    "            correct += 1\n",
    "        # print(polity, \": \", seshat_answer, response['answer'])\n",
    "        total += 1\n",
    "    percentage = correct / total * 100\n",
    "    print(f\"Correct: {correct}, Total: {total}, Percentage: {percentage:.2f}%\")\n",
    "    return percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance(polities_with_scientific_literatures_df, scientific_literature_responses, 'scientific_literature')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try that again\n",
    "\n",
    "Now let's run the pipeline again with a different variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seshat_api.sc import DrinkingWaterSupplies\n",
    "drinking_water_supplies = DrinkingWaterSupplies(client)\n",
    "drinking_water_supplies_df = pd.DataFrame(drinking_water_supplies.get_all())\n",
    "polities_with_drinking_water_supplies_df = process_df(drinking_water_supplies_df, 'drinking_water_supply_system')\n",
    "polities_with_drinking_water_supplies_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the responses for the Drinking Water Supply System variable\n",
    "polities_with_drinking_water_supplies_responses = deepseek_responses(polities_with_drinking_water_supplies_df,\n",
    "                                                     'Drinking Water Supply System',\n",
    "                                                     \"Talking about Specialized Buildings, drinking water supply systems are polity owned (which includes owned by the community, or the state), we have coded the absence or presence of the variable\",\n",
    "                                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the responses to a CSV file\n",
    "polities_with_drinking_water_supplies_responses.to_csv('polities_with_drinking_water_supplies_responses.csv', index=False)\n",
    "\n",
    "# Load the responses from the CSV file\n",
    "# polities_with_drinking_water_supplies_responses = pd.read_csv('polities_with_drinking_water_supplies_responses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polities_with_drinking_water_supplies_responses.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance(polities_with_drinking_water_supplies_df, polities_with_drinking_water_supplies_responses, 'drinking_water_supply_system')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seshat_api.sc import Roads\n",
    "roads = Roads(client)\n",
    "roads_df = pd.DataFrame(roads.get_all())\n",
    "polities_with_roads_df = process_df(roads_df, 'road')\n",
    "polities_with_roads_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the responses for the Roads variable\n",
    "polities_with_roads_responses = deepseek_responses(polities_with_roads_df,\n",
    "                                                     'Roads',\n",
    "                                                     \"Talking about Transport infrastructure, roads refers to deliberately constructed roads that connect settlements or other sites. It excludes streets/accessways within settlements and paths between settlements that develop through repeated use\"\n",
    "                                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the responses to a CSV file\n",
    "polities_with_roads_responses.to_csv('polities_with_roads_responses.csv', index=False)\n",
    "\n",
    "# Load the responses from the CSV file\n",
    "# polities_with_roads_responses = pd.read_csv('polities_with_roads_responses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polities_with_roads_responses.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance(polities_with_roads_df, polities_with_roads_responses, 'road')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seshat_api.wf import Irons\n",
    "irons = Irons(client)\n",
    "irons_df = pd.DataFrame(irons.get_all())\n",
    "polities_with_irons_df = process_df(irons_df, 'iron')\n",
    "polities_with_irons_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the responses for the Military use of Metals: Iron variable\n",
    "polities_with_irons_responses = deepseek_responses(polities_with_irons_df,\n",
    "                                                     'Military use of Metals: Iron',\n",
    "                                                     \"The absence or presence of iron as a military technology used in warfare\"\n",
    "                                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the responses to a CSV file\n",
    "polities_with_irons_responses.to_csv('polities_with_irons_responses.csv', index=False)\n",
    "\n",
    "# Load the responses from the CSV file\n",
    "# polities_with_irons_responses = pd.read_csv('polities_with_irons_responses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polities_with_irons_responses.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance(polities_with_irons_df, polities_with_irons_responses, 'iron')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Using DeepSeek-R1, as an example of a recent language model with advanced capabilities, we have provided a series of prompts to test its knowledge of history, using expert reviewed data from the Seshat Global History data as a ground truth. We have specifically asked a set of questions that would require some level of reasoning, rather than just knowledge. Since the model was run locally, it was unable to query online resources, all answers coming directly from the model itself.\n",
    "\n",
    "The questions are all about social complexity and military technology, and pertain to variables for which data has been collected in Seshat, across a range of historical polities. These polities vary by era and geography, but the only information we gave the LLM was the polity name and the years in which it was active. The prompts read something like this:\n",
    "\n",
    "> \"Use your knowledge of world history to answer the following question. Given your knowledge of the historical polity **'Fatimid Caliphate'**, a polity that existed between **909 CE** and **1171 CE**, do you expect that **Scientific Literature** was *present* or *absent*? **Scientific Literature** is defined as: **'Talking about Kinds of Written Documents, Scientific literature includes mathematics, natural sciences, social sciences'**. Answer *'XXXpresentXXX'* if you expect it to be present, and *'XXXabsentXXX'* if you expect it to be absent.\"\n",
    "\n",
    "We then extracted the answers from DeepSeek's responses. The results showed that the number of correct answers was not far from **50%**, which is what you might expect if it was picking at random given the binary choice of \"absent\" and \"present\" (other values such as \"transitional\" and \"unknown\" exist for these variables in Seshat data, but we didn't include those polities here).\n",
    "\n",
    "In the CSV example answers I have saved in this repository, the results in terms of correct answers were:\n",
    "- Scientific Literature: 52.67% (79/150) \n",
    "- Drinking Water Supply System: 46.43% (52/112)\n",
    "- Roads: 58.82% (110/187) \n",
    "- Military use of Metals - Iron: 62.02% (209/337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add whether answer was correct to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correctness(polities_df, responses_df, variable_id):\n",
    "    # Add a new column to the responses_df to store the correct answer\n",
    "    responses_df['correct'] = responses_df.apply(lambda row: polities_df[polities_df['new_name'] == row['new_name']][variable_id].values[0], axis=1)\n",
    "    return responses_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scientific_literature_responses = correctness(polities_with_scientific_literatures_df, scientific_literature_responses, 'scientific_literature')\n",
    "polities_with_drinking_water_supplies_responses = correctness(polities_with_drinking_water_supplies_df, polities_with_drinking_water_supplies_responses, 'drinking_water_supply_system')\n",
    "polities_with_roads_responses = correctness(polities_with_roads_df, polities_with_roads_responses, 'road')\n",
    "polities_with_irons_responses = correctness(polities_with_irons_df, polities_with_irons_responses, 'iron')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-save the responses to a CSVs\n",
    "scientific_literature_responses.to_csv('scientific_literature_responses.csv', index=False)\n",
    "polities_with_drinking_water_supplies_responses.to_csv('polities_with_drinking_water_supplies_responses.csv', index=False)\n",
    "polities_with_roads_responses.to_csv('polities_with_roads_responses.csv', index=False)\n",
    "polities_with_irons_responses.to_csv('polities_with_irons_responses.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scientific_literature_responses[scientific_literature_responses['new_name'] == 'tr_ottoman_emp_1']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (seshat_api_1)",
   "language": "python",
   "name": "seshat_api_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

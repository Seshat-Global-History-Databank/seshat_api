{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seshat_api import SeshatAPI\n",
    "import pandas as pd\n",
    "from ollama import chat, ChatResponse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions for DeepSeek-R1\n",
    "\n",
    "Can a recent top performing LLM (DeepSeek-R1) correctly predict whether a variable from the Seshat Global History Databank (e.g. \"Scientific Literature\") should be \"present\" or \"absent\" for a selection of polities, given a definition of the variable, the name of the polity and the years in which it existed?\n",
    "\n",
    "If DeepSeek has a good understanding of history, can it use this to guess from the polity name and years what time/place the prompt is referring to, and whether the variable in question would have been present?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = SeshatAPI(base_url=\"https://seshat-db.com/api\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data from Seshat\n",
    "\n",
    "First let's define our variable to be used in an LLM prompt later (taken from seshat-db.com), then load the data from the Seshat API to use as our ground truth to test against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = 'Scientific Literature'\n",
    "definition = \"Talking about Kinds of Written Documents, Scientific literature includes mathematics, natural sciences, social sciences\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seshat_api.sc import ScientificLiteratures\n",
    "scientific_literatures = ScientificLiteratures(client)\n",
    "scientific_literatures_df = pd.DataFrame(scientific_literatures.get_all())\n",
    "len(scientific_literatures_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just use expert reviewed data and ignore examples where the value is anything other than \"present\" or \"absent\" to create a subsample of the dataset.\n",
    "\n",
    "We should also reformat the dataframe so we have information about the polities such as the start and end year alongside the variable value, then remove columns we aren't interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(seshat_df, variable_id):\n",
    "    # Filter out the records that are not expert reviewed\n",
    "    seshat_df = seshat_df[seshat_df['expert_reviewed'] == True]\n",
    "    print(len(seshat_df))\n",
    "\n",
    "    # Filter out records where the variable is not either 'present' or 'absent'\n",
    "    seshat_df = seshat_df[seshat_df[variable_id].isin(['present', 'absent'])]\n",
    "    print(len(seshat_df))\n",
    "\n",
    "    # Extract the polities column to a new dataframe\n",
    "    polities_df = pd.DataFrame(seshat_df['polity'].tolist())\n",
    "\n",
    "    # Add the scientific_literature column to the new dataframe\n",
    "    polities_df[variable_id] = seshat_df[variable_id]\n",
    "    print(len(polities_df))\n",
    "\n",
    "    # Filter out records where scientific_literature is NaN\n",
    "    polities_df = polities_df[polities_df[variable_id].notna()]\n",
    "    print(len(polities_df))\n",
    "\n",
    "    # Get rid of the columns we don't need\n",
    "    polities_df = polities_df[['new_name', 'long_name', 'start_year', 'end_year', variable_id, 'general_description']]\n",
    "\n",
    "    return polities_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polities_with_scientific_literatures_df = process_df(scientific_literatures_df, 'scientific_literature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polities_with_scientific_literatures_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Prompt engineering\"\n",
    "\n",
    "First let's define a prompt function to use with our dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_CE(year):\n",
    "    if year >= 0:\n",
    "        return f\"{year} CE\"\n",
    "    else:\n",
    "        return f\"{abs(year)} BCE\"\n",
    "\n",
    "def prompt_func(seshat_df, polity_name, variable, variable_definition):\n",
    "    df = seshat_df[seshat_df['new_name'] == polity_name]\n",
    "    polity = list(df['long_name'])[0]\n",
    "    # description = list(df['general_description'])[0]  # TODO: we could add this to the prompt to add more context\n",
    "    start_year = list(df['start_year'])[0]\n",
    "    end_year = list(df['end_year'])[0]\n",
    "    prompt = \"Use your knowledge of world history to answer the following question. \"\n",
    "    prompt += f\"Given your knowledge of the historical polity '{polity}', \"\n",
    "    prompt += f\"a polity that existed between {year_CE(start_year)} and {year_CE(end_year)}\"\n",
    "    prompt += f\", do you expect that {variable} was present or absent? \"\n",
    "    prompt += f\"{variable} is defined as: '{variable_definition}'. \"\n",
    "    \n",
    "    # To help extract the answer from the text response later, make sure we have a string that can be found with a regex:\n",
    "    prompt += \"Answer 'XXXpresentXXX' if you expect it to be present, and 'XXXabsentXXX' if you expect it to be absent.\"\n",
    "    return prompt\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the prompt look with an example \"new_name\" (Seshat ID) of `tn_fatimid_cal`, which we know has a record for scientific literature in the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seshat_polity_name = 'tn_fatimid_cal'\n",
    "test_prompt = prompt_func(polities_with_scientific_literatures_df, test_seshat_polity_name, variable, definition)\n",
    "test_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's see what DeepSeek responds for this prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response: ChatResponse = chat(model='deepseek-r1', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': test_prompt,\n",
    "  },\n",
    "])\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was that correct? Let's check the dataframe to see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polities_with_scientific_literatures_df[polities_with_scientific_literatures_df['new_name'] == test_seshat_polity_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get DeepSeek's answer data\n",
    "\n",
    "Let's write a function to collect responses from DeepSeek for all the polities in our dataset and make a new dataframe with the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepseek_responses(seshat_df, variable, definition, count):  # Use the count parameter to limit the number of responses\n",
    "    def generate_response(polity):\n",
    "        response: ChatResponse = chat(model='deepseek-r1', messages=[\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': prompt_func(seshat_df, polity, variable, definition),\n",
    "            },\n",
    "        ])\n",
    "        return response\n",
    "\n",
    "    responses = pd.DataFrame(columns=['new_name', 'answer', 'full'])\n",
    "    for polity in seshat_df['new_name']:\n",
    "        if len(responses) >= count:\n",
    "            break\n",
    "        response = generate_response(polity)\n",
    "        answer = \"\"\n",
    "        try:\n",
    "            answer = re.search(r'(XXXabsentXXX|XXXpresentXXX)', response.message.content).group(1)\n",
    "        except:\n",
    "            print(\"No answer found for\", polity)\n",
    "        answer = answer.replace(\"XXX\", \"\")\n",
    "        print(polity, \": \", answer)\n",
    "\n",
    "        # Add the new row to the DataFrame using pd.concat\n",
    "        responses = pd.concat([responses, pd.DataFrame([{\n",
    "            'new_name': polity,\n",
    "            'answer': answer,\n",
    "            'full': response.message.content\n",
    "        }])], ignore_index=True)\n",
    "\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate the answer data for this variable across polities in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the responses for the scientific literature variable\n",
    "scientific_literature_responses = deepseek_responses(polities_with_scientific_literatures_df, variable, definition, 2)\n",
    "scientific_literature_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the responses to a CSV file\n",
    "scientific_literature_responses.to_csv('scientific_literature_responses.csv', index=False)\n",
    "\n",
    "# Load the responses from the CSV file\n",
    "# scientific_literature_responses = pd.read_csv('scientific_literature_responses.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How well did the model perform?\n",
    "\n",
    "Let's do a simple check to see what percentage of the answers were correct, according to the Seshat ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance(polities_df, responses_df, variable_id):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for _, response in responses_df.iterrows():\n",
    "        polity = response['new_name']\n",
    "        seshat_answer = polities_df[polities_df['new_name'] == polity][variable_id].values[0]\n",
    "        if seshat_answer == response['answer']:\n",
    "            correct += 1\n",
    "        # print(polity, \": \", seshat_answer, response['answer'])\n",
    "        total += 1\n",
    "    percentage = correct / total * 100\n",
    "    print(f\"Correct: {correct}, Total: {total}, Percentage: {percentage:.2f}%\")\n",
    "    return percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance(polities_with_scientific_literatures_df, scientific_literature_responses, 'scientific_literature')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try that again\n",
    "\n",
    "Now let's run the pipeline again with a different variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (seshat_api_1)",
   "language": "python",
   "name": "seshat_api_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
